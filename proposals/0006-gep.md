---
MEP: 0006
Title: Chat Streaming Support
Discussion:
Implementation:
---

# Chat Streaming Support

## Abstract

Glide will support chat streaming endpoints as this is important feature with AI Chatbots. Additionally, this helps with latency issues seen with non-streaming responses.

## Motivation

Streaming of LLM responses is an important UX feature for AI chatbots. As chatbot use cases increase in complexity,
response times from LLMs also increase. Streaming provide an enhanced UX as the end user actively sees the LLM generating the response.
Additionally, Glide has the intention to support other streaming uses cases such as speech-to-text and text-to-speech.
These intial efforts will support additional streaming use cases.

### Requirements

- R1: Streaming should be supported via the standard `v1/language` endpoint.
- R2: Streaming should be enabled by default.
- R3: Streaming should be specified in the Config if non-default values are needed.
- R4: Streaming should be handled at the router level. A router is either streaming or not streaming.
- R5: Initially, failures should be handled via a retry of the entire input prompt/message.
- R6: A seperate `doChatRequest` will be built to handle streaming.

## Design

[TBU, all information about the proposed solution, possibly evaluations against the requirements]

## Alternatives Considered

[TBU, what other solutions were considered and why they were rejected]

## Future Work

- Dynamic retry: When a failure occurs try to use a different LLM to complete the output without retrying the entire input.
