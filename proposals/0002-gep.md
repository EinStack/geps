---
MEP: 0002
Title: Unifed API
Discussion: Link
Implementation: Link
---

# TITLE: Unified API

## Abstract

This GEP discusses the unified API that handles the interaction with LLM providers.

## Motivation

[TBU, what problem we are trying to solve, focus on users, use cases, and all important context]

The Unified API holds all the logic needed to create a request to a provider. This solution allows the clients to be easily
envoked by a provider pool.


### Requirements

- R1: User should be able to specify specific models or select auto mode (selection handled by Glide)
- R2: Should be a Hertz Client
- R3: Should take a user request as input
- R4: Handles all provider specific logic
- R5: Easily maintained


## Design


- A Hertz client that handles an incoming request and returns a reponse from that provider.
- Provider specific clients

### Routes
```yaml
routes:
    chat: api/v1/chat
    complete: /api/v1/complete #this is also know as generate in some providers

    # want to avoid specifying a provider pool as in the future we may
    # want to combine strategies e.g User employs a fallback strategy where
    # the fallback model is the one with the lowest latency
```

#### User Request Schema for Chat Route

Example Request Body in YAML for readability & comments

```yaml
mode: fallback # selected strategy, required(?)
globalParams: # Selected model inherits these where applicable, optional
  temperature: 0.7
  max_tokens: 100
models:
  - provider: openai # Ideally this field is optional but for MVP it may need to be required I think
    model: gpt-3.5-turbo # optional
    message: "tell me a joke" # required
    params: # provider specific params
      messages:
        - role: system
          content: "You are a helpful assistant."
        - role: user
          content: "tell me a joke" # can this be inherited from 'message'
  - provider: cohere # everything below here is optional as we can auto select a backup and pass the global params to it
    model: command-light
    message: "tell me a joke" # inherits from first option if left blank
    params:
      connectors:
        id: web-search
```

Example Request Body:

```json
{
  "mode": "fallback",
  "globalParams": {
    "temperature": 0.7,
    "max_tokens": 100
  },
  "models": [
    {
      "provider": "openai",
      "model": "gpt-3.5-turbo",
      "message": "tell me a joke",
      "params": {
        "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "tell me a joke"
          }
        ]
      }
    },
    {
      "provider": "cohere",
      "model": "command-light",
      "message": "tell me a joke",
      "params": {
        "connectors": {
          "id": "web-search"
        }
      }
    }
  ]
}

```

### Provider Request Schemas

Request schema from OpenAI Client to OpenAI API

```json

{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "tell me a joke"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 100,
}

```

### Response Schema For Chat

Response Schema from OpenAI API to user

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-3.5-turbo-0613",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "\n\nHello there, how may I assist you today?",
    },
    "logprobs": null,
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}

```

## Example

User request calls for a fallback strategy of  OpenAI backed up by Anthropic. OpenAI is available so the OpenAI client is invoked and OpenAI reuqest is built from data in the user request.

## Reference

LangchainGo: https://github.com/tmc/langchaingo/tree/main

## Alternatives Considered

[TBU, what other solutions were considered and why they were rejected]

## Future Work

[TBU, what we put outside of the equation now, but may want to consider in the future]
