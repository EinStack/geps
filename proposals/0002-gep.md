---
MEP: 0002
Title: Unifed API
Discussion: Link
Implementation: Link
---

# Unified API

## Abstract

This GEP discusses the unified API that handles the interaction with LLM providers. This represents the feature set for MVP.

## Motivation


The Unified API abstracts much of the overhead of interacting with LLM providers.
It stores all the provider specific logic in a single place and allowing developers to focus on the core logic. It also introduces durability to LLM apps. Additionally, unifying the API allows optimizations to be leveraged such as latency and cost improvements.


### Requirements

- R1: User should be able to specify specific models or select auto mode (selection handled by Glide)
- R2: Should be a Hertz Client
- R3: Should take a user request as input
- R4: Handles all provider specific logic
- R5: Easily maintained
- R6: API schemas must unify common LLM request params (e.g. topP, N, etc)
- R7: API routes must unify common LLM endpoints/API
- R8: API schemas should allow to customize provider-specific params (e.g. a different messages for OpenAI/Anthropic)
- R9: Focus on building out a single endpoint for /Chat. Most apps will be using this and then we can focus our efforts on making this part of our API super effective
- R10: Disallow overrides in the MVP. All params are updated via config. This will change over time but I want users to tell which params they want to override (and how) instead of us making the assumption.


## Design


- A Hertz client that handles an incoming request and returns a reponse from that provider.
- Provider specific clients

### Routes
```yaml
routes:
    chat: /v1/language/{pool-id}/chat/ # or /v1/llms/...
    transcribers: /v1/transcribers/{pool-id}/  # ...
    speech-synthesizer:  /v1/synthesizers/{pool-id}/ # ...
    multi-modal: /v1/multi/{pool-id}/multimodal/ # future route, not for MVP
    embedding: /v1/embeddings/{pool-id}/embed/ # future route, not for MVP
```


#### User Request Schema for Chat Route

Below is an example user request to the Chat route. The other routes will look very similar, if not identical. The provider specific params will look different.

Unified API schema:

```json
{ 
  "provider": "cohere",
  "model": "command-light",
  "params": {"temperature": 0.7, "topP": 1.0, "frequencyPenalty": 0.0, "presencePenalty": 0.0, "max_tokens": 256, "n": 1},
  "message": "tell me a joke",
  "messageHistory": ["Hello there", "How are you?", "I'm good, how about you?"]
}
```

```go

type UnifiedAPIData struct {
	Provider       string                  `json:"provider"`
	Model          string                  `json:"model"`
	APIKey         string                  `json:"api_key"`
	Params         map[string]interface{}  `json:"params"`
	Message        map[string]string       `json:"message"`
	MessageHistory []map[string]string     `json:"messageHistory"`
}

```

Example OpenAI Request Body:

```json
{
  "provider": "openai",
  "model": "gpt-3.5-turbo",
  "params": {"temperature": 0.7, "topP": 1.0, "frequencyPenalty": 0.0, "presencePenalty": 0.0, "max_tokens": 256, "n": 1},
  "message": [
      {
        "role": "user",
        "content": "Where was it played?"
      }
    ],
  "messageHistory": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who won the world series in 2020?"},
    {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
  ]
}
```

Example Cohere Request Body:


```json
{
  "message": "tell me a joke",
  "messageHistory": ["Hello there", "How are you?", "I'm good, how about you?"]
}
```

Proposed override schema:

```json
{
  "message": "tell me a joke",
  "messageHistory": ["Hello there", "How are you?", "I'm good, how about you?"],
  "override": {
    "prompt": "You are a witty assistant",
    "temperature": 0.7,
    "topP": 1
  }
}
```

### Provider Request Schemas

OpenAi Chat Endpoint Request Schema

```json

{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "tell me a joke"
      }
    ]
}

```
Cohere Chat Endpoint Request Schema (required params only)

```json
{
  "message": "Please explain to me how LLMs work"
}
```

Cohere Generate Endpoint Request Schema (required params only)

```json
{
  "prompt": "Please explain to me how LLMs work"
}
```

### Response Schema For Chat

For MVP, lets just return the response from the provider. Once we get feedback we can incorporate more metedata from the gateway about the request.

Response Schema from OpenAI API to user

```json
{
  "provider": "openai",
  "pool": "fallback",
  "provider_response": {
    "object": "chat.completion",
    "created": 1677652288,
    "model": "gpt-3.5-turbo-0613",
    "system_fingerprint": "fp_44709d6fcb",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "\n\nHello there, how may I assist you today?"
        },
        "logprobs": null,
        "finish_reason": "stop"
      }
    ],
    "usage": {
      "prompt_tokens": 9,
      "completion_tokens": 12,
      "total_tokens": 21
    }
  }
}


```

Response Schema from Cohere API to user

```json
{
  "provider": "cohere",
  "pool": "latency",
  "provider_response": {
    "response_id": "12af01ff-ee43-44f6-bdf9-b5aa14a6f738",
    "text": "Isaac Newton was born on January 4, 1643. However, there is some debate over his birth date, with some records suggesting he was born on December 25, 1642, or even as early as October 24, 1642. Is there anything else you would like to know about Sir Isaac Newton?",
    "generation_id": "8a345fc4-afa6-4ddf-8a1a-f601edb6c82b",
    "token_count": {
      "prompt_tokens": 92,
      "response_tokens": 144,
      "total_tokens": 236,
      "billed_tokens": 218
    },
    "meta": {
      "api_version": {
        "version": "1"
      },
      "billed_units": {
        "input_tokens": 74,
        "output_tokens": 144
      }
    }
  }
}


```

## Example

In a RAG use case:

- The LLM dev indexes their data in a vectorDB

- The LLM dev crafts prompt, search for relevant context in the vectorDB and adds it to the prompt. Also, they experiment with params and prompt to find a set that works the best

- Up to this point, this was experiments. Now it's time to make that app more production ready. The LLM dev deploys/uses hosted vectorDB. Along with that, they deploy Glide with one default provider pool. The pool contains one model they experimented

- The Q&A app uses Glide client to work with LLM. It also maintains the conversational memory and sends it as soon as a conversation develops. The app uses /v1/default/chat for that, for example.

- To make it more resilient, the LLM dev adds two more LLMs in the default pool.

- After more experiments and testing, turned out that one of fallback models in the pool works badly with the prompt used for the main model. The LLM dev overrides the prompt for that model, so now each time the application sends request to Glide, it specifies two prompts: the main one (used by two models) and one more for one of the fallback models.

## Reference

LangchainGo: https://github.com/tmc/langchaingo/tree/main
LLMLite Config: https://github.com/BerriAI/litellm/blob/main/litellm/proxy/proxy_server.py

## Future Work

- Additional routes (multi-model, embedding)
- Additional providers
- Streaming responses
