---
MEP: 0002
Title: Unifed API
Discussion: Link
Implementation: Link
---

# TITLE: Unified API

## Abstract

This GEP discusses the unified API that handles the interaction with LLM providers. This represents the feature set for MVP.

## Motivation


The Unified API abstracts much of the overhead of interacting with LLM providers.
It stores all the provider specific logic in a single place and allowing developers to focus on the core logic. It also introduces durability to LLM apps. Additionally, unifying the API allows optimizations to be leveraged such as latency and cost improvements.


### Requirements

- R1: User should be able to specify specific models or select auto mode (selection handled by Glide)
- R2: Should be a Hertz Client
- R3: Should take a user request as input
- R4: Handles all provider specific logic
- R5: Easily maintained
- R6: API schemas must unify common LLM request params (e.g. topP, N, etc)
- R7: API routes must unify common LLM endpoints/API
- R8: API schemas should allow to customize provider-specific params (e.g. a different messages for OpenAI/Anthropic)


## Design


- A Hertz client that handles an incoming request and returns a reponse from that provider.
- Provider specific clients

### Routes
```yaml
routes:
    chat: /chat/{pool-id}
    complete: /complete/{pool-id} #this is also know as generate in some providers
    multi-modal: /multimodal/{pool-id} # future route, not for MVP
    embedding: /embedding/{pool-id} # future route, not for MVP
```

#### User Request Schema for Chat Route

Below is an example user request to the Chat route. The other routes will look very similar, if not identical. The provider specific params will look different.

Example Request Body:


```json
{
  "message": "tell me a joke",
}
```

### Provider Request Schemas

Request schema from OpenAI Client to OpenAI API

```json

{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "tell me a joke"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 100,
}

```

### Response Schema For Chat

For MVP, lets just return the response from the provider. Once we get feedback we can incorporate more metedata from the gateway about the request.

Response Schema from OpenAI API to user

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-3.5-turbo-0613",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "\n\nHello there, how may I assist you today?",
    },
    "logprobs": null,
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}

```

## Example

In a RAG use case:

- The LLM dev indexes their data in a vectorDB

- The LLM dev crafts prompt, search for relevant context in the vectorDB and adds it to the prompt. Also, they experiment with params and prompt to find a set that works the best

- Up to this point, this was experiments. Now it's time to make that app more production ready. The LLM dev deploys/uses hosted vectorDB. Along with that, they deploy Glide with one default provider pool. The pool contains one model they experimented

- The Q&A app uses Glide client to work with LLM. It also maintains the conversational memory and sends it as soon as a conversation develops. The app uses /v1/default/chat for that, for example.

- To make it more resilient, the LLM dev adds two more LLMs in the default pool.

- After more experiments and testing, turned out that one of fallback models in the pool works badly with the prompt used for the main model. The LLM dev overrides the prompt for that model, so now each time the application sends request to Glide, it specifies two prompts: the main one (used by two models) and one more for one of the fallback models.

## Reference

LangchainGo: https://github.com/tmc/langchaingo/tree/main

## Future Work

- Additional routes (multi-model, embedding)
- Additional providers
- Streaming responses
